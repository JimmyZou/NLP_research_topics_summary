# README #

Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation [[link]](https://arxiv.org/abs/1703.09902)  
Neural Machine Translation and Sequence-to-sequence Models: A Tutorial [[link]](https://arxiv.org/abs/1703.01619)  
Comparative Study of CNN and RNN for Natural Language Processing [[link]](https://arxiv.org/pdf/1702.01923.pdf)  
A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks [[link]](A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks)  


# Word Embedding and Reading Comprehension
## text/document classification
Get To The Point: Summarization with Pointer-Generator Networks [[link]](https://arxiv.org/abs/1704.04368v2)  
Neural Extractive Summarization with Side Information [[link]](https://arxiv.org/abs/1704.04530)  
A Survey of Neural Network Techniques for Feature Extraction from Text [[link]](https://arxiv.org/abs/1704.08531v1)  
Learning to Skim Text [[link]](https://arxiv.org/abs/1704.06877)  
An Actor-Critic Algorithm for Sequence Prediction [[link]](https://arxiv.org/abs/1607.07086v3)  
DeepMind Generative and Discriminative Text Classification with Recurrent Neural Networks [[link]](https://arxiv.org/pdf/1703.01898.pdf)  
Learning Word Vectors for Sentiment Analysis [[link]](http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)  
A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification [[link]](https://arxiv.org/abs/1510.03820)  
Very Deep Convolutional Networks for Text ClassiÔ¨Åcation [[link]](https://arxiv.org/abs/1606.01781) [[code]](https://github.com/geduo15/Very-Deep-Convolutional-Networks-for-Natural-Language-Processing-in-tensorflow)  
Character-level Convolutional Networks for Text Classification [[link]](https://arxiv.org/abs/1509.01626)  
Text Understanding from Scratch [[link]](https://arxiv.org/abs/1502.01710v5) [[code]](https://github.com/zhangxiangxiao/Crepe)  
Hierarchical Attention Networks for Document Classification [[link]](http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) [[code1]](https://github.com/richliao/textClassifier) [[code2]](https://github.com/EdGENetworks/attention-networks-for-classification)  
FastText (Facebook AI Research) [[github]](https://github.com/facebookresearch/fastText)  
Bag of Tricks for Efficient Text Classification [[link]](https://arxiv.org/abs/1607.01759)  
Learning text representation using recurrent convolutional neural network with highway layers [[link]](https://arxiv.org/pdf/1606.06905.pdf) [[code]](https://github.com/wenying45/deep_learning_tutorial/blob/master/rcnn-hw/RCNN-HW-IMDB.ipynb)  
### text classification dataset
summary on googledrive [[link]](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M)  
Movie Review Data [[link]](http://www.cs.cornell.edu/people/pabo/movie%2Dreview%2Ddata/)  
AG's corpus of news articles [[link]](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)  
SogouCA and SogouCS news corpora [[link]](http://www.sogou.com/labs/resource/list_news.php)  
DBPedia ontology dataset [[link]](http://wiki.dbpedia.org/services-resources/datasets/nlp)  
Yelp reviews [[link]](https://en.yelp.com.hk/dataset_challenge/dataset)  
Yahoo! Answers dataset  
Amazon reviews [[link]](http://snap.stanford.edu/data/web-Amazon.html)  

## word/text representation
Learning word embeddings efficiently with noise-contrastive estimation [[link]](http://www.gatsby.ucl.ac.uk/~amnih/papers/wordreps.pdf)  
Enriching Word Vectors with Subword Information [[link]](https://arxiv.org/abs/1607.04606)  
Incremental Skip-gram Model with Negative Sampling [[link]](https://arxiv.org/pdf/1704.03956.pdf)  
Glove: Global Vectors for Word Representation [[link]](http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/PenningtonSocherManning2014.pdf)  
All-but-the-Top: Simple and Effective Postprocessing for Word Representations [[link]](https://arxiv.org/abs/1702.01417)  
A Comparative Study of Word Embeddings for Reading Comprehension [[link]](https://arxiv.org/abs/1703.00993)  

## text comprehension
ReasoNet: Learning to Stop Reading in Machine Comprehension [[link]](https://arxiv.org/abs/1609.05284v1)  
SQuAD: 100,000+ Questions for Machine Comprehension of Text [[link]](https://arxiv.org/abs/1606.05250)  
Language as a Latent Variable: Discrete Generative Models for Sentence Compression [[Link]](https://arxiv.org/pdf/1609.07317v1.pdf)  
Text understanding with the attention sum reader network [[link]](https://arxiv.org/abs/1603.01547)  
Scaffolding Networks for Teaching and Learning to Comprehend [[link]](https://arxiv.org/abs/1702.08653)  
Words Or Characters? Fine-Grained Gating For Reading Comprehension [[link]](https://arxiv.org/pdf/1611.01724v1.pdf)  
A Comparative Study of Word Embeddings for Reading Comprehension [[link]](https://arxiv.org/abs/1703.00993)  

# Neutral Machine Translation 
(first Attention in NMT)Neural Machine Translation by Jointly Learning to Align and Translate [[link]](https://arxiv.org/abs/1409.0473v7)  
Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation [[link]](https://arxiv.org/abs/1611.04558v1)  
Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot [[link]](https://arxiv.org/abs/1611.04503v1)  
Variational neural machine translation EMNLP2016 [[link]](https://arxiv.org/pdf/1605.07869.pdf)  
Dual Learning for Machine Translation [[link]](https://arxiv.org/abs/1611.00179)  
Neural Machine Translation with Reconstruction [[link]](https://arxiv.org/pdf/1611.01874v2.pdf)  
Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets [[link]](https://arxiv.org/abs/1703.04887)  
Massive Exploration of Neural Machine Translation Architectures [[link]](https://arxiv.org/abs/1703.03906) [[code]](https://github.com/google/seq2seq/)  
Nematus: a Toolkit for Neural Machine Translation [[link]](https://arxiv.org/pdf/1703.04357.pdf) [[code]](https://github.com/rsennrich/nematus)  

# Dialogue Systems and Chatbot
Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction [[link]](https://arxiv.org/abs/1704.01691v1)  
Personalizing a Dialogue System with Transfer Learning [[link]](https://arxiv.org/abs/1610.02891v2)  
A Knowledge-Grounded Neural Conversation Model [[link]](https://arxiv.org/pdf/1702.01932.pdf)  
A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues [[link]](https://arxiv.org/pdf/1605.06069.pdf)  
End-to-End Reinforcement Learning of Dialogue Agents for Information Access [[link]](https://arxiv.org/abs/1609.00777v2)  
End-to-End Task-Completion Neural Dialogue Systems [[link]](https://arxiv.org/abs/1703.01008) [[code]](https://github.com/MiuLab/TC-Bot)  
A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue [[link]](https://arxiv.org/pdf/1701.04024.pdf)  
Learning end-to-end goal oriented dialog [[link]](https://openreview.net/pdf?id=S1Bb3D5gg)  
Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders [[link]](https://arxiv.org/pdf/1703.10960.pdf)  
FRAMES: A corpus for adding memory to goal oriented dialogue systems [[link]](https://arxiv.org/pdf/1704.00057.pdf)  
Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory [[link]](https://arxiv.org/abs/1704.01074)  
## Q&A
Learning to Compose Neural Networks for Question Answering [[link]](https://arxiv.org/abs/1601.01705v4)  
Semi-Supervised QA with Generative Domain-Adaptive Nets [[link]](https://arxiv.org/abs/1702.02206v1)  
Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering [[link]](https://arxiv.org/pdf/1703.04617.pdf)  
Reading Wikipedia to Answer Open-Domain Questions [[link]](https://arxiv.org/abs/1704.00051)  
Paraconsistency and Word Puzzles [[link]](https://arxiv.org/abs/1608.01338)  



# Information Extraction
Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning [[link]]()  
Automated Phrase Mining from Massive Text Corpora [[link]](https://arxiv.org/abs/1702.04457v2)  
Pointing the Unknown Words [[link]](https://arxiv.org/abs/1603.08148v3)  

# GAN
## GAN for NLP
Context-aware Natural Language Generation with Recurrent Neural Networks [[link]](https://arxiv.org/abs/1611.09900v1)  
Chinese Song Iambics Generation with Neural Attention-based Model [[link]](https://arxiv.org/abs/1604.06274v2)  
SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[link]](https://arxiv.org/pdf/1609.05473.pdf)  
Deep reinforcement learning for dialogue generation [[link]](https://arxiv.org/abs/1606.01541)  
Generating Text via Adversarial Training [[link]](http://people.duke.edu/~yz196/pdf/textgan.pdf)  
Adversarial Learning for Neural Dialogue Generation [[link]](https://arxiv.org/pdf/1701.06547.pdf)  
Connecting generative adversarial network and actor-critic methods [[link]](https://arxiv.org/pdf/1610.01945.pdf)  
A Hybrid Convolutional Variational Autoencoder for Text Generation [[link]](https://arxiv.org/pdf/1702.02390.pdf)  
Neural Variational Inference for Text Processing [[link]](https://arxiv.org/pdf/1511.06038.pdf)  
Generating Sentences From a Continuous Spaces [[link]](https://aclweb.org/anthology/K/K16/K16-1002.pdf)  
Generalization and Equilibrium in Generative Adversarial Nets (GANs) [[link]](https://arxiv.org/abs/1703.00573)  
BEGAN: Boundary Equilibrium Generative Adversarial Networks [[link]](https://arxiv.org/abs/1703.10717)  
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities [[link]](https://arxiv.org/abs/1701.06264) [[blog1]](https://zhuanlan.zhihu.com/p/25204020) [[blog2]](https://zhuanlan.zhihu.com/p/25580027) [[code1]](https://github.com/guojunq/lsgan) [[code2]](https://github.com/guojunq/glsgan)  

# DNN theories
Exploring Sparsity in Recurrent Neural Networks [[link]](https://arxiv.org/abs/1704.05119)  
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis [[ink]](https://arxiv.org/abs/1702.04595)  
Introspection: Accelerating Neural Network Training By Learning Weight Evolution [[link]](https://arxiv.org/abs/1704.04959v1)  
Structured Attention Networks [[link]](https://arxiv.org/abs/1702.00887) [[code]](https://github.com/harvardnlp/struct-attn)  
LightRNN: Memory and Computation-Efficient Recurrent Neural Networks [[link]](https://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks.pdf)  
Learning Gradient Descent: Better Generalization and Longer Horizons [[link]](https://arxiv.org/abs/1703.03633) [[code]](https://github.com/vfleaking/rnnprop)  
Guided Perturbations: Self Corrective Behavior in Convolutional Neural Networks [[link]](https://arxiv.org/abs/1703.07928)  
Identifying Beneficial Task Relations for Multi-task Learning in Deep Neural Networks [[link]](https://arxiv.org/abs/1702.08303) [[code]](https://github.com/jbingel/eacl2017_mtl)  
## RL theory
Combining policy gradient and Q-learning [[link]](https://arxiv.org/abs/1611.01626)  
An Actor-Critic Algorithm for Sequence Prediction [[link]](https://arxiv.org/abs/1607.07086v3)  

# Computer vision
Hierarchical Memory Networks [[link]](https://arxiv.org/abs/1605.07427v1)  
Language Modeling with Gated Convolutional Networks [[link]](https://arxiv.org/abs/1612.08083v1)  
Sequence-to-Sequence Learning as Beam-Search Optimization [[link]](https://arxiv.org/abs/1606.02960v2)  

## Image Caption
MAT: A Multimodal Attentive Translator for Image Captioning [[link]](https://arxiv.org/abs/1702.05658v1)  
Self-critical Sequence Training for Image Captioning [[link]](https://arxiv.org/abs/1612.00563)  
Review Networks for Caption Generation [[link]](https://arxiv.org/abs/1605.07912)  
An Empirical Study of Language CNN for Image Captioning [[link]](https://arxiv.org/pdf/1612.07086v2.pdf)  
Visual Dialog [[link]](https://arxiv.org/abs/1611.08669v2)  
Show, Attend and Tell: Neural Image Caption Generation with Visual Attention [[link]](https://arxiv.org/abs/1502.03044v3)  
Neural Image Caption Generation with Visual Attention [[link]](https://arxiv.org/pdf/1502.03044v3.pdf)  

## Semantic Segmentation
Predicting Deeper into the Future of Semantic Segmentation [[link]](https://arxiv.org/abs/1703.07684)  
Coupled Deep Learning for Heterogeneous Face Recognition [[link]](https://arxiv.org/pdf/1704.02450.pdf)  
AMR-to-text Generation with Synchronous Node Replacement Grammar [[link]](https://arxiv.org/pdf/1702.00500v3.pdf)  
Semantic Segmentation Using Adversarial Networks [[link]](https://arxiv.org/abs/1611.08408)  

