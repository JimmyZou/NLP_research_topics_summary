# README #
# Summary papers
Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation [[link]](https://arxiv.org/abs/1703.09902)  
Neural Machine Translation and Sequence-to-sequence Models: A Tutorial [[link]](https://arxiv.org/abs/1703.01619)  
Comparative Study of CNN and RNN for Natural Language Processing [[link]](https://arxiv.org/pdf/1702.01923.pdf)  
A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks [[link]](https://arxiv.org/abs/1611.01587)  

# Word Embedding and Text Comprehension
## text/sentence classification
Convolutional Sequence to Sequence Learning [[link]](https://arxiv.org/pdf/1705.03122.pdf)  
Learning to Skim Text [[link]](https://arxiv.org/abs/1704.06877)  
Generative and Discriminative Text Classification with Recurrent Neural Networks [[link]](https://arxiv.org/pdf/1703.01898.pdf)  
Very Deep Convolutional Networks for Text ClassiÔ¨Åcation [[link]](https://arxiv.org/abs/1606.01781) [[code]](https://github.com/geduo15/Very-Deep-Convolutional-Networks-for-Natural-Language-Processing-in-tensorflow)  
Hierarchical Attention Networks for Document Classification [[link]](http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) [[code1]](https://github.com/richliao/textClassifier) [[code2]](https://github.com/EdGENetworks/attention-networks-for-classification)  
FastText (Facebook AI Research) [[github]](https://github.com/facebookresearch/fastText)  
Bag of Tricks for Efficient Text Classification [[link]](https://arxiv.org/abs/1607.01759)  
Learning text representation using recurrent convolutional neural network with highway layers [[link]](https://arxiv.org/pdf/1606.06905.pdf) [[code]](https://github.com/wenying45/deep_learning_tutorial/blob/master/rcnn-hw/RCNN-HW-IMDB.ipynb)  
Character-level Convolutional Networks for Text Classification [[link]](https://arxiv.org/abs/1509.01626)  
Text Understanding from Scratch [[link]](https://arxiv.org/abs/1502.01710v5) [[code]](https://github.com/zhangxiangxiao/Crepe)  
A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification [[link]](https://arxiv.org/abs/1510.03820)  
## text extractive/abstractive summarization
Neural Extractive Summarization with Side Information [[link]](https://arxiv.org/abs/1704.04530)  
Get To The Point: Summarization with Pointer-Generator Networks [[link]](https://arxiv.org/abs/1704.04368v2)  
SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents [[link]](https://arxiv.org/abs/1611.04230)  
Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond [[link]](https://arxiv.org/pdf/1602.06023v5.pdf)  
Sequence-to-Sequence RNNs for Text Summarization [[link]](https://arxiv.org/abs/1602.06023v1) [[code]](https://github.com/tensorflow/models/tree/master/textsum)  
Neural Headline Generation on Abstract Meaning Representation [[link]](http://aclweb.org/anthology/D/D16/D16-1112.pdf)  
A Neural Attention Model for Abstractive Sentence Summarization [[link]](https://arxiv.org/abs/1509.00685v2)  
## others
Learning to Identify Ambiguous and Misleading News Headlines[[link]](https://arxiv.org/abs/1705.06031)  
### text dataset
CNN / Daily Mail dataset (non-anonymized) for summarization[[link]](https://github.com/abisee/cnn-dailymail)  


## word representation
Adversarial Multi-Criteria Learning for Chinese Word Segmentation [[link]](https://arxiv.org/abs/1704.07556)  
A Survey of Neural Network Techniques for Feature Extraction from Text [[link]](https://arxiv.org/abs/1704.08531v1)  
Incremental Skip-gram Model with Negative Sampling [[link]](https://arxiv.org/pdf/1704.03956.pdf)  
A Comparative Study of Word Embeddings for Reading Comprehension [[link]](https://arxiv.org/abs/1703.00993)  
All-but-the-Top: Simple and Effective Postprocessing for Word Representations [[link]](https://arxiv.org/abs/1702.01417)  
The Stanford CoreNLP Natural Language Processing Toolkit [[link]](https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf)  
Learning Word Vectors for Sentiment Analysis [[link]](http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)  
Learning word embeddings efficiently with noise-contrastive estimation [[link]](http://www.gatsby.ucl.ac.uk/~amnih/papers/wordreps.pdf)  
Enriching Word Vectors with Subword Information [[link]](https://arxiv.org/abs/1607.04606)  
Glove: Global Vectors for Word Representation [[link]](http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/PenningtonSocherManning2014.pdf)  

## text comprehension and generation
A Comparative Study of Word Embeddings for Reading Comprehension [[link]](https://arxiv.org/abs/1703.00993)  
Scaffolding Networks for Teaching and Learning to Comprehend [[link]](https://arxiv.org/abs/1702.08653)  
An Actor-Critic Algorithm for Sequence Prediction [[link]](https://arxiv.org/abs/1607.07086v3)  
ReasoNet: Learning to Stop Reading in Machine Comprehension [[link]](https://arxiv.org/abs/1609.05284v1)  
SQuAD: 100,000+ Questions for Machine Comprehension of Text [[link]](https://arxiv.org/abs/1606.05250)  
Language as a Latent Variable: Discrete Generative Models for Sentence Compression [[Link]](https://arxiv.org/pdf/1609.07317v1.pdf)  
Text understanding with the attention sum reader network [[link]](https://arxiv.org/abs/1603.01547)  
Words Or Characters? Fine-Grained Gating For Reading Comprehension [[link]](https://arxiv.org/pdf/1611.01724v1.pdf)  
Teaching Machines to Read and Comprehend [[link]](https://arxiv.org/pdf/1506.03340.pdf)  

# Information Extraction
Mining Quality Phrases from Massive Text Corpora [[link]](http://jialu.cs.illinois.edu/paper/sigmod2015-liu.pdf)  
Automated Phrase Mining from Massive Text Corpora [[link]](https://arxiv.org/abs/1702.04457v2) [[code]](https://github.com/shangjingbo1226/AutoPhrase)  
Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning [[link]](https://arxiv.org/abs/1603.07954v3)  
Pointing the Unknown Words [[link]](https://arxiv.org/abs/1603.08148v3)  

# GAN
Domain-Adversarial Training of Neural Networks [[link]](https://arxiv.org/abs/1505.07818v4) [[code]](https://github.com/pumpikano/tf-dann)  
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks [[link]](https://arxiv.org/abs/1511.06434v2) [[code]](https://github.com/carpedm20/DCGAN-tensorflow)  
Adversarial discriminative domain adaptation [[link]](https://arxiv.org/abs/1702.05464)  
## GAN for NLP
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities [[link]](https://arxiv.org/abs/1701.06264) [[blog1]](https://zhuanlan.zhihu.com/p/25204020) [[blog2]](https://zhuanlan.zhihu.com/p/25580027) [[code1]](https://github.com/guojunq/lsgan) [[code2]](https://github.com/guojunq/glsgan)  
A Hybrid Convolutional Variational Autoencoder for Text Generation [[link]](https://arxiv.org/pdf/1702.02390.pdf)  
Adversarial Learning for Neural Dialogue Generation [[link]](https://arxiv.org/pdf/1701.06547.pdf)  
Generalization and Equilibrium in Generative Adversarial Nets (GANs) [[link]](https://arxiv.org/abs/1703.00573)  
BEGAN: Boundary Equilibrium Generative Adversarial Networks [[link]](https://arxiv.org/abs/1703.10717)  
Context-aware Natural Language Generation with Recurrent Neural Networks [[link]](https://arxiv.org/abs/1611.09900v1)  
Chinese Song Iambics Generation with Neural Attention-based Model [[link]](https://arxiv.org/abs/1604.06274v2)  
SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[link]](https://arxiv.org/pdf/1609.05473.pdf)  
Deep reinforcement learning for dialogue generation [[link]](https://arxiv.org/abs/1606.01541)  
Generating Text via Adversarial Training [[link]](http://people.duke.edu/~yz196/pdf/textgan.pdf)  
Connecting generative adversarial network and actor-critic methods [[link]](https://arxiv.org/pdf/1610.01945.pdf)  
Neural Variational Inference for Text Processing [[link]](https://arxiv.org/pdf/1511.06038.pdf)  
Generating Sentences From a Continuous Spaces [[link]](https://aclweb.org/anthology/K/K16/K16-1002.pdf)  

# Neutral Machine Translation 
(first Attention in NMT)Neural Machine Translation by Jointly Learning to Align and Translate [[link]](https://arxiv.org/abs/1409.0473v7)  
Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation [[link]](https://arxiv.org/abs/1611.04558v1)  
Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot [[link]](https://arxiv.org/abs/1611.04503v1)  
Variational neural machine translation EMNLP2016 [[link]](https://arxiv.org/pdf/1605.07869.pdf)  
Dual Learning for Machine Translation [[link]](https://arxiv.org/abs/1611.00179)  
Neural Machine Translation with Reconstruction [[link]](https://arxiv.org/pdf/1611.01874v2.pdf)  
Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets [[link]](https://arxiv.org/abs/1703.04887)  
Massive Exploration of Neural Machine Translation Architectures [[link]](https://arxiv.org/abs/1703.03906) [[code]](https://github.com/google/seq2seq/)  
Nematus: a Toolkit for Neural Machine Translation [[link]](https://arxiv.org/pdf/1703.04357.pdf) [[code]](https://github.com/rsennrich/nematus)  

# Dialogue Systems and Chatbot
Multi-agent cooperation and the emergence of (Natural) language [[link]](https://arxiv.org/pdf/1612.07182.pdf)  
Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction [[link]](https://arxiv.org/abs/1704.01691v1)  
Personalizing a Dialogue System with Transfer Learning [[link]](https://arxiv.org/abs/1610.02891v2)  
A Knowledge-Grounded Neural Conversation Model [[link]](https://arxiv.org/pdf/1702.01932.pdf)  
A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues [[link]](https://arxiv.org/pdf/1605.06069.pdf)  
End-to-End Reinforcement Learning of Dialogue Agents for Information Access [[link]](https://arxiv.org/abs/1609.00777v2)  
End-to-End Task-Completion Neural Dialogue Systems [[link]](https://arxiv.org/abs/1703.01008) [[code]](https://github.com/MiuLab/TC-Bot)  
A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue [[link]](https://arxiv.org/pdf/1701.04024.pdf)  
Learning end-to-end goal oriented dialog [[link]](https://openreview.net/pdf?id=S1Bb3D5gg)  
Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders [[link]](https://arxiv.org/pdf/1703.10960.pdf)  
FRAMES: A corpus for adding memory to goal oriented dialogue systems [[link]](https://arxiv.org/pdf/1704.00057.pdf)  
Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory [[link]](https://arxiv.org/abs/1704.01074)  
Key-Value Retrieval Networks for Task-Oriented Dialogue [[link]](https://arxiv.org/pdf/1705.05414.pdf)  

# QA SQuAD
Teaching Machines to Read and Comprehend [[link]](https://arxiv.org/abs/1506.03340)  
Learning to Compose Neural Networks for Question Answering [[link]](https://arxiv.org/abs/1601.01705v4)  
SQuAD: 100,000+ Questions for Machine Comprehension of Text [[link]](https://arxiv.org/abs/1606.05250v3)  
Paraconsistency and Word Puzzles [[link]](https://arxiv.org/abs/1608.01338)  
Machine Comprehension Using Match-LSTM and Answer Pointer [[link]](https://arxiv.org/abs/1608.07905)  
Bidirectional Attention Flow for Machine Comprehension [[link]](https://arxiv.org/abs/1611.01603)  
Dynamic Coattention Networks For Question Answering [[link]](https://arxiv.org/abs/1611.01604)  
Multi-Perspective Context Matching for Machine Comprehension [[link]](https://arxiv.org/abs/1612.04211)  
Semi-Supervised QA with Generative Domain-Adaptive Nets [[link]](https://arxiv.org/abs/1702.02206v1)  
Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering [[link]](https://arxiv.org/pdf/1703.04617.pdf)  
Reading Wikipedia to Answer Open-Domain Questions [[link]](https://arxiv.org/abs/1704.00051)  
Making Neural QA as Simple as Possible but not Simpler [[link]](https://arxiv.org/abs/1703.04816v2)  
R-net: Machine Reading Comprehension With Self-Matching Networks[[link]](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf)  

# DNN theory
Convolutional Sequence to Sequence Learning [[link]](https://arxiv.org/abs/1705.03122)  
Understanding deep learning requires rethinking generalization [[link]](https://arxiv.org/abs/1611.03530)  
Exploring Sparsity in Recurrent Neural Networks [[link]](https://arxiv.org/abs/1704.05119)  
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis [[ink]](https://arxiv.org/abs/1702.04595)  
Introspection: Accelerating Neural Network Training By Learning Weight Evolution [[link]](https://arxiv.org/abs/1704.04959v1)  
Structured Attention Networks [[link]](https://arxiv.org/abs/1702.00887) [[code]](https://github.com/harvardnlp/struct-attn)  
LightRNN: Memory and Computation-Efficient Recurrent Neural Networks [[link]](https://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks.pdf)  
Learning Gradient Descent: Better Generalization and Longer Horizons [[link]](https://arxiv.org/abs/1703.03633) [[code]](https://github.com/vfleaking/rnnprop)  
Guided Perturbations: Self Corrective Behavior in Convolutional Neural Networks [[link]](https://arxiv.org/abs/1703.07928)  
Identifying Beneficial Task Relations for Multi-task Learning in Deep Neural Networks [[link]](https://arxiv.org/abs/1702.08303) [[code]](https://github.com/jbingel/eacl2017_mtl)  

# RL theory
Hybrid computing using a neural network with dynamic external memory [[link]](http://web.stanford.edu/class/psych209/Readings/GravesWayne16DNC.pdf)  
Combining policy gradient and Q-learning [[link]](https://arxiv.org/abs/1611.01626)  
An Actor-Critic Algorithm for Sequence Prediction [[link]](https://arxiv.org/abs/1607.07086v3)  

# Computer vision
Hierarchical Memory Networks [[link]](https://arxiv.org/abs/1605.07427v1)  
Language Modeling with Gated Convolutional Networks [[link]](https://arxiv.org/abs/1612.08083v1)  
Sequence-to-Sequence Learning as Beam-Search Optimization [[link]](https://arxiv.org/abs/1606.02960v2)  

## Image Caption
MAT: A Multimodal Attentive Translator for Image Captioning [[link]](https://arxiv.org/abs/1702.05658v1)  
Self-critical Sequence Training for Image Captioning [[link]](https://arxiv.org/abs/1612.00563)  
Review Networks for Caption Generation [[link]](https://arxiv.org/abs/1605.07912)  
An Empirical Study of Language CNN for Image Captioning [[link]](https://arxiv.org/pdf/1612.07086v2.pdf)  
Visual Dialog [[link]](https://arxiv.org/abs/1611.08669v2)  
Show, Attend and Tell: Neural Image Caption Generation with Visual Attention [[link]](https://arxiv.org/abs/1502.03044v3)  
Neural Image Caption Generation with Visual Attention [[link]](https://arxiv.org/pdf/1502.03044v3.pdf)  

## Semantic Segmentation
Predicting Deeper into the Future of Semantic Segmentation [[link]](https://arxiv.org/abs/1703.07684)  
Coupled Deep Learning for Heterogeneous Face Recognition [[link]](https://arxiv.org/pdf/1704.02450.pdf)  
AMR-to-text Generation with Synchronous Node Replacement Grammar [[link]](https://arxiv.org/pdf/1702.00500v3.pdf)  
Semantic Segmentation Using Adversarial Networks [[link]](https://arxiv.org/abs/1611.08408)  


